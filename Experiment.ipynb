{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from END import EnsembleND\n",
    "import NestedDichotomies.nd as nd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from PairwiseCoupling import PairwiseCoupling\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Baselearner\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Methods\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Basics\n",
    "import pandas as pd\n",
    "import os\n",
    "from threading import Thread\n",
    "import openml\n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(y_predict, y_test, nclass):\n",
    "    obj_num = np.size(y_test)\n",
    "    bs_ytrue = np.zeros((obj_num,nclass))\n",
    "    for i in range(obj_num):\n",
    "        bs_ytrue[i,y_test[i]]=1\n",
    "    bs = sum(sum((y_predict-bs_ytrue)**2))/obj_num\n",
    "    return bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelcombos.END_DT import EnsembleND_DT as END_DT\n",
    "from modelcombos.END_LR import EnsembleND_LR as END_LR\n",
    "from modelcombos.END_NB import EnsembleND_NB as END_NB\n",
    "from modelcombos.PC_DT import PairwiseCoupling_DT as PC_DT\n",
    "from modelcombos.PC_LR import PairwiseCoupling_LR as PC_LR\n",
    "from modelcombos.PC_NB import PairwiseCoupling_NB as PC_NB\n",
    "def generateModelName(model):\n",
    "    name = 'error'\n",
    "    if (model.__class__==RandomForestClassifier):\n",
    "        name = 'RF'\n",
    "    if (model.__class__==PC_DT):\n",
    "        name = 'PC_DT'\n",
    "    if (model.__class__==PC_LR):\n",
    "        name = 'PC_LR'\n",
    "    if (model.__class__==PC_NB):\n",
    "        name = 'PC_NB'\n",
    "    if (model.__class__==MLPClassifier):\n",
    "        name = 'MLP'\n",
    "    if (model.__class__==END_DT):\n",
    "        name = 'END_DT'\n",
    "    if (model.__class__==END_LR):\n",
    "        name = 'END_LR'\n",
    "    if (model.__class__==END_NB):\n",
    "        name = 'END_NB'\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score_singular_factory(nclass):\n",
    "    #def brier_score_singular(y_test, y_predict):\n",
    "    #    bs_ytrue = np.zeros((nclass))\n",
    "    #    bs_ytrue[y_test]=1\n",
    "    #    bs = sum((y_predict-bs_ytrue)**2)\n",
    "    #    return bs\n",
    "    def brier_score_singular(y_test,y_predict):\n",
    "        #print(y_predict)\n",
    "        #print(y_test)\n",
    "        obj_num = np.size(y_test)\n",
    "        bs_ytrue = np.zeros((obj_num,nclass))\n",
    "        for i in range(obj_num):\n",
    "            bs_ytrue[i,y_test[i]]=1\n",
    "        bs = sum(sum((y_predict-bs_ytrue)**2))/obj_num\n",
    "        return bs\n",
    "    \n",
    "    return brier_score_singular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TMTB_and_ECE(y_predict, y_test, nclass, nbins=10, ccstrat='uniform'):\n",
    "    y_pred_list = np.reshape(y_predict,nclass*y_test.size)\n",
    "    ninst = y_test.size\n",
    "    onehot = np.zeros((ninst, nclass))\n",
    "    onehot[np.arange(ninst), y_test] = 1\n",
    "    y_test_list = np.reshape(onehot,nclass*y_test.size)\n",
    "    prob_true, prob_pred = calibration_curve(y_test_list, y_pred_list, n_bins=nbins, strategy=ccstrat)\n",
    "    ece = np.sum(np.absolute(prob_true-prob_pred))/prob_true.size\n",
    "    return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCSVofData(path, file_name, data, columns):\n",
    "    df = pd.DataFrame(np.array(data),columns=columns)\n",
    "    df.to_csv(path+file_name+'.csv', sep='\\t', encoding='utf-8', index=False)\n",
    "    \n",
    "def saveResults(dataset_name, bs_byModel_base_run, ece_byModel_base_run, bs_byModel_sig_run, ece_byModel_sig_run, bs_byModel_iso_run, ece_byModel_iso_run, hyperparam_byModel_run, mft_byModel_run, mst_byModel_run):\n",
    "    dir_path = os.getcwd()\n",
    "    directory = dir_path+'/experiments/'+dataset_name+'/result_'+strftime(\"%Y-%m-%d %H_%M_%S\", gmtime())+'/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    models = generate_ModelHyperparam_pairs()\n",
    "    m_names = [generateModelName(models[i][0]) for i in range(len(models))]\n",
    "    saveCSVofData(directory,'brier_base', bs_byModel_base_run, m_names)\n",
    "    saveCSVofData(directory,'ece_base', ece_byModel_base_run, m_names)\n",
    "    saveCSVofData(directory,'brier_sigmoid', bs_byModel_sig_run, m_names)\n",
    "    saveCSVofData(directory,'ece_sigmoid', ece_byModel_sig_run, m_names)\n",
    "    saveCSVofData(directory,'brier_isotonic', bs_byModel_iso_run, m_names)\n",
    "    saveCSVofData(directory,'ece_isotonic', ece_byModel_iso_run, m_names)\n",
    "    saveCSVofData(directory,'hyperparameter', hyperparam_byModel_run, m_names)\n",
    "    saveCSVofData(directory,'mean_fit_time', mft_byModel_run, m_names)\n",
    "    saveCSVofData(directory,'mean_score_time', mst_byModel_run, m_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Compare all models\n",
    "def CompareModels(data_X, data_y, n_runs=1, n_jobs=1):\n",
    "    seed = 2000\n",
    "    num_classes = np.unique(data_y).size\n",
    "    # for each run the brierscore for each model [no calibration]\n",
    "    bs_byModel_base_run = []\n",
    "    # for each run the ece (expected calibration error) for each model [no calibration]\n",
    "    ece_byModel_base_run = []\n",
    "    # for each run the brierscore for each model [sigmoid calibration]\n",
    "    bs_byModel_sig_run = []\n",
    "    # for each run the ece (expected calibration error) for each model [sigmoid calibration]\n",
    "    ece_byModel_sig_run = []\n",
    "    # for each run the brierscore for each model [isotonic calibration]\n",
    "    bs_byModel_iso_run = []\n",
    "    # for each run the ece (expected calibration error) for each model [isotonic calibration]\n",
    "    ece_byModel_iso_run = []\n",
    "    # for each run the hyperparameter for each model\n",
    "    hyperparam_byModel_run = []\n",
    "    # for each run the mean fit time for each model\n",
    "    mft_byModel_run = []\n",
    "    # for each run the mean score time for each model\n",
    "    mst_byModel_run = []\n",
    "    # RUNS\n",
    "    for i in range(n_runs):\n",
    "        print('run',i+1)\n",
    "        bs_byModel_base_run.append([])\n",
    "        ece_byModel_base_run.append([])\n",
    "        bs_byModel_sig_run.append([])\n",
    "        ece_byModel_sig_run.append([])\n",
    "        bs_byModel_iso_run.append([])\n",
    "        ece_byModel_iso_run.append([])\n",
    "        hyperparam_byModel_run.append([])\n",
    "        mft_byModel_run.append([])\n",
    "        mst_byModel_run.append([])\n",
    "        # train - test von data (80/20)\n",
    "        X_train, X_test, y_train, y_test = tts(data_X, data_y, test_size=0.2, stratify=data_y, random_state=seed)\n",
    "        # model - calibration split von train (70/30)\n",
    "        X_model, X_calibration, y_model, y_calibration = tts(X_train, y_train, test_size=0.3, stratify=y_train, random_state=seed+1)\n",
    "        \n",
    "        scoring = {'bs': make_scorer(brier_score_singular_factory(num_classes), greater_is_better=False, needs_proba=True)}\n",
    "        \n",
    "        model_dists = generate_ModelHyperparam_pairs(num_classes, seed)\n",
    "        \n",
    "        for k in range(len(model_dists)):\n",
    "            print('model',k+1)\n",
    "            #if (k==1):\n",
    "            #    set_trace()\n",
    "            model_rs = RandomizedSearchCV(model_dists[k][0], param_distributions=model_dists[k][1], scoring=scoring,refit='bs', n_iter = 10, n_jobs = n_jobs, cv=3, random_state=seed)\n",
    "            model_rs.fit(X_model, y_model)\n",
    "            print('mean fit time:',model_rs.cv_results_['mean_fit_time'].mean())\n",
    "            print('mean score time:',model_rs.cv_results_['mean_score_time'].mean())\n",
    "            seed += 1\n",
    "            print(model_rs.best_params_)\n",
    "            # best estimator\n",
    "            hyperparam_byModel_run[i].append(model_rs.best_params_)\n",
    "            model = model_rs.best_estimator_\n",
    "            #calibration sigmoid\n",
    "            c_sig_model = CalibratedClassifierCV(base_estimator=model,method='sigmoid', cv='prefit')\n",
    "            c_sig_model.fit(X_calibration, y_calibration)\n",
    "            #calibration isotonic\n",
    "            c_iso_model = CalibratedClassifierCV(base_estimator=model,method='isotonic', cv='prefit')\n",
    "            c_iso_model.fit(X_calibration, y_calibration)\n",
    "            #prediction base\n",
    "            y_pred_base = model.predict_proba(X_test)\n",
    "            bs_base = brier_score(y_predict=y_pred_base, y_test=y_test, nclass=num_classes)\n",
    "            ece_base = TMTB_and_ECE(y_predict=y_pred_base, y_test=y_test, nclass=num_classes, nbins=10, ccstrat='uniform')\n",
    "            #prediction sigmoid calibrated model\n",
    "            y_pred_sig = c_sig_model.predict_proba(X_test)\n",
    "            bs_sig = brier_score(y_predict=y_pred_sig, y_test=y_test, nclass=num_classes)\n",
    "            ece_sig = TMTB_and_ECE(y_predict=y_pred_sig, y_test=y_test, nclass=num_classes, nbins=10, ccstrat='uniform')\n",
    "            #prediction isotonic calibrated model\n",
    "            y_pred_iso = c_iso_model.predict_proba(X_test)\n",
    "            bs_iso = brier_score(y_predict=y_pred_iso, y_test=y_test, nclass=num_classes)\n",
    "            ece_iso = TMTB_and_ECE(y_predict=y_pred_iso, y_test=y_test, nclass=num_classes, nbins=10, ccstrat='uniform')\n",
    "            #append results\n",
    "            bs_byModel_base_run[i].append(bs_base)\n",
    "            ece_byModel_base_run[i].append(ece_base)\n",
    "            bs_byModel_sig_run[i].append(bs_sig)\n",
    "            ece_byModel_sig_run[i].append(ece_sig)\n",
    "            bs_byModel_iso_run[i].append(bs_iso)\n",
    "            ece_byModel_iso_run[i].append(ece_iso)\n",
    "            mft_byModel_run[i].append(model_rs.cv_results_['mean_fit_time'].mean())\n",
    "            mst_byModel_run[i].append(model_rs.cv_results_['mean_score_time'].mean())\n",
    "            print(bs_base,ece_base,bs_sig, ece_sig, bs_iso, ece_iso)\n",
    "    return bs_byModel_base_run, ece_byModel_base_run, bs_byModel_sig_run, ece_byModel_sig_run, bs_byModel_iso_run, ece_byModel_iso_run, hyperparam_byModel_run, mft_byModel_run, mst_byModel_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelcombos.END_DT import EnsembleND_DT as END_DT\n",
    "from modelcombos.END_LR import EnsembleND_LR as END_LR\n",
    "from modelcombos.END_NB import EnsembleND_NB as END_NB\n",
    "from modelcombos.PC_DT import PairwiseCoupling_DT as PC_DT\n",
    "from modelcombos.PC_LR import PairwiseCoupling_LR as PC_LR\n",
    "from modelcombos.PC_NB import PairwiseCoupling_NB as PC_NB\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "def generate_ModelHyperparam_pairs(nclasses=3, seed=42):\n",
    "    models = []\n",
    "    RFC_paramdist =  {\n",
    "        'min_impurity_decrease': uniform(0.00001, 0.1),\n",
    "        'min_samples_leaf': randint(1,51)}\n",
    "    models.append((RandomForestClassifier(n_estimators=45, random_state=seed), RFC_paramdist))\n",
    "    DT_paramdist = {\n",
    "        'max_depth': randint(1,4)\n",
    "    }\n",
    "    LR_paramdist = {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': uniform(0.01,20)\n",
    "    }\n",
    "    NB_paramdist = {\n",
    "        'var_smoothing' : uniform(0.0000000001,1.0)\n",
    "    }\n",
    "    models.append((END_DT(number_of_nds=5, number_of_classes=nclasses, max_depth = 1, generator_String='random_pair', random_state=seed), DT_paramdist))\n",
    "    models.append((END_LR(number_of_nds=5, number_of_classes=nclasses,penalty='l2', C=1.0, generator_String='random_pair', random_state=seed), LR_paramdist))\n",
    "    models.append((END_NB(number_of_nds=5, number_of_classes=nclasses, var_smoothing=0.00001, generator_String='random_pair', random_state=seed),NB_paramdist))\n",
    "    models.append((PC_DT(classes=nclasses, seed=seed, max_depth=1),DT_paramdist))\n",
    "    models.append((PC_LR(classes=nclasses, seed=seed, penalty='l2', C=1.0),LR_paramdist))\n",
    "    models.append((PC_NB(classes=nclasses, seed=seed, var_smoothing=0.0001),NB_paramdist))\n",
    "    MLP_paramdist = {\n",
    "        'alpha': uniform(0.00000001, 10),\n",
    "        'batch_size': [100,200,300,400,500],\n",
    "        'power_t': uniform(0.001,5) \n",
    "    }\n",
    "    models.append((MLPClassifier(hidden_layer_sizes = (35,), activation='logistic', learning_rate='invscaling'), MLP_paramdist))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "    \n",
    "def write_single_value_per_run(file, models, value_name, values):\n",
    "    file.write(value_name+'\\n')\n",
    "    file.write(generateModelName(models[0][0]))\n",
    "    for i in range(len(models)-1):\n",
    "        file.write('\\t'+generateModelName(models[i+1][0]))\n",
    "    file.write('\\n')\n",
    "    for i in range(len(values)):\n",
    "        file.write(str(values[i][0]))\n",
    "        for k in range(len(values[i])-1):\n",
    "            file.write('\\t'+str(values[i][k+1]))\n",
    "        file.write('\\n')\n",
    "    \n",
    "def save_result(dataset_name, nclasses, bs_byModel_base_run, ece_byModel_base_run, bs_byModel_sig_run, ece_byModel_sig_run, bs_byModel_iso_run, ece_byModel_iso_run, hyperparam_byModel_run, mft_byModel_run, mst_byModel_run):\n",
    "    dir_path = os.getcwd()\n",
    "    directory = dir_path+'/experiments/'+dataset_name+'/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    models = generate_ModelHyperparam_pairs(nclasses=nclasses, seed=42)\n",
    "    file = open(directory+'result_'+strftime(\"%Y-%m-%d %H_%M_%S\", gmtime())+'.txt', 'w')\n",
    "    # WRITE MODEL DESCRIPTIONS\n",
    "    try:\n",
    "        # SAVING BRIER SCORE\n",
    "        file.write('Brier-Score\\n')\n",
    "        file.write(generateModelName(models[0][0]))\n",
    "        for i in range(len(models)-1):\n",
    "            file.write('\\t\\t\\t'+generateModelName(models[i+1][0]))\n",
    "        file.write('\\nBase\\tSigmoid\\tIsotonic')\n",
    "        for i in range(len(models)-1):\n",
    "            file.write('\\tBase\\tSigmoid\\tIsotonic')\n",
    "        file.write('\\n')\n",
    "        for i in range(len(bs_byModel_base_run)):\n",
    "            file.write(str(bs_byModel_base_run[i][0])+'\\t'+str(bs_byModel_sig_run[i][0])+'\\t'+str(bs_byModel_iso_run[i][0]))\n",
    "            for k in range(len(bs_byModel_base_run[i])-1):\n",
    "                file.write('\\t'+str(bs_byModel_base_run[i][k+1])+'\\t'+str(bs_byModel_sig_run[i][k+1])+'\\t'+str(bs_byModel_iso_run[i][k+1]))\n",
    "            file.write('\\n')\n",
    "        # SAVING ECE SCORE\n",
    "        file.write('ECE-Score\\n')\n",
    "        file.write(generateModelName(models[0][0]))\n",
    "        for i in range(len(models)-1):\n",
    "            file.write('\\t\\t\\t'+generateModelName(models[i+1][0]))\n",
    "        file.write('\\nBase\\tSigmoid\\tIsotonic')\n",
    "        for i in range(len(models)-1):\n",
    "            file.write('\\tBase\\tSigmoid\\tIsotonic')\n",
    "        file.write('\\n')\n",
    "        for i in range(len(ece_byModel_base_run)):\n",
    "            file.write(str(ece_byModel_base_run[i][0])+'\\t'+str(ece_byModel_sig_run[i][0])+'\\t'+str(ece_byModel_iso_run[i][0]))\n",
    "            for k in range(len(ece_byModel_base_run[i])-1):\n",
    "                file.write('\\t'+str(ece_byModel_base_run[i][k+1])+'\\t'+str(ece_byModel_sig_run[i][k+1])+'\\t'+str(ece_byModel_iso_run[i][k+1]))\n",
    "            file.write('\\n')\n",
    "        # SAVING HYPERPARAMETER\n",
    "        write_single_value_per_run(file, models, 'Hyperparameter', hyperparam_byModel_run)\n",
    "        # SAVING MEAN FIT TIME\n",
    "        write_single_value_per_run(file, models, 'Mean Fit Time', mft_byModel_run)\n",
    "        # SAVING MEAN SCORE TIME\n",
    "        write_single_value_per_run(file, models, 'Mean Score Time', mst_byModel_run)\n",
    "    finally:\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1515 - micro-mass\n",
    "#1459 - artificial-characters\n",
    "#1233 - eating\n",
    "#1569 - poker-hand\n",
    "#1503 - spoken-arabic-digit\n",
    "#4541 - Diabetes130US\n",
    "#4538 - GesturePhaseSegmentationProcessed\n",
    "#41991 - Kuzushiji-49 - error\n",
    "#40670 - dna\n",
    "#1478 - har\n",
    "#40984 - segment\n",
    "#40498 - wine-quality-white\n",
    "#40499 - texture\n",
    "#40686 - solar-flare\n",
    "#41972 - Indian_pines\n",
    "#1475 - first-order-theorem-proving\n",
    "#id_list = [1515,1459,1233,1569,1503,4541,4538,41991,40670,1478,40984,40498,40499,40686,41972,1475]\n",
    "#for i in range(len(id_list)):\n",
    "#    ID_n = id_list[i]\n",
    "#    RUNS_n = 20\n",
    "#    JOBS_n = 10\n",
    "#    run_dataset(ID_n, RUNS_n, JOBS_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start experiment\n",
    "def run_dataset(ID, RUNS, JOBS):\n",
    "    dataset = openml.datasets.get_dataset(ID)\n",
    "    dataset_name = dataset.name\n",
    "    print(dataset_name)\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(dataset_format='array', target=dataset.default_target_attribute)\n",
    "    num_classes = np.unique(y).size\n",
    "    #bs & ece for (b)ase, (s)igmoid and (i)sotonic\n",
    "    bs_b, ece_b, bs_s, ece_s, bs_i, ece_i, hyppar, mft, mst = CompareModels(data_X=X, data_y=y, n_runs=RUNS, n_jobs=JOBS)\n",
    "    saveResults(dataset_name=dataset_name, bs_byModel_base_run=bs_b, ece_byModel_base_run=ece_b, bs_byModel_sig_run=bs_s, ece_byModel_sig_run=ece_s, bs_byModel_iso_run=bs_i, ece_byModel_iso_run=ece_i, hyperparam_byModel_run=hyppar, mft_byModel_run=mft, mst_byModel_run=mst)\n",
    "    #save_result(dataset_name=dataset_name, nclasses=num_classes, bs_byModel_base_run=bs_b, ece_byModel_base_run=ece_b, bs_byModel_sig_run=bs_s, ece_byModel_sig_run=ece_s, bs_byModel_iso_run=bs_i, ece_byModel_iso_run=ece_i, hyperparam_byModel_run=hyppar, mft_byModel_run=mft, mst_byModel_run=mst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [1515,1459,1233,1569,1503,4541,4538,41991,40670,1478,40984,40498,40499,40686,41972,1475]\n",
    "for i in range(len(id_list)):\n",
    "    ID_n = id_list[i]\n",
    "    RUNS_n = 20\n",
    "    JOBS_n = 10\n",
    "    run_dataset(ID_n, RUNS_n, JOBS_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
