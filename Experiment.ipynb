{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from END import EnsembleND\n",
    "import NestedDichotomies.nd as nd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from PairwiseCoupling import PairwiseCoupling\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Baselearner\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Methods\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Basics\n",
    "import os\n",
    "from threading import Thread\n",
    "import openml\n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(y_predict, y_test, nclass):\n",
    "    obj_num = np.size(y_test)\n",
    "    bs_ytrue = np.zeros((obj_num,nclass))\n",
    "    for i in range(obj_num):\n",
    "        bs_ytrue[i,y_test[i]]=1\n",
    "    bs = sum(sum((y_predict-bs_ytrue)**2))/obj_num\n",
    "    return bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelcombos.END_DT import EnsembleND_DT as END_DT\n",
    "from modelcombos.END_LR import EnsembleND_LR as END_LR\n",
    "from modelcombos.END_NB import EnsembleND_NB as END_NB\n",
    "from modelcombos.PC_DT import PairwiseCoupling_DT as PC_DT\n",
    "from modelcombos.PC_LR import PairwiseCoupling_LR as PC_LR\n",
    "from modelcombos.PC_NB import PairwiseCoupling_NB as PC_NB\n",
    "def generateModelName(model):\n",
    "    name = 'error'\n",
    "    if (model.__class__==RandomForestClassifier):\n",
    "        name = 'RF'\n",
    "    if (model.__class__==PC_DT):\n",
    "        name = 'PC_DT'\n",
    "    if (model.__class__==PC_LR):\n",
    "        name = 'PC_LR'\n",
    "    if (model.__class__==PC_NB):\n",
    "        name = 'PC_NB'\n",
    "    if (model.__class__==MLPClassifier):\n",
    "        name = 'MLP'\n",
    "    if (model.__class__==END_DT):\n",
    "        name = 'END_DT'\n",
    "    if (model.__class__==END_LR):\n",
    "        name = 'END_LR'\n",
    "    if (model.__class__==END_NB):\n",
    "        name = 'END_NB'\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score_singular_factory(nclass):\n",
    "    #def brier_score_singular(y_test, y_predict):\n",
    "    #    bs_ytrue = np.zeros((nclass))\n",
    "    #    bs_ytrue[y_test]=1\n",
    "    #    bs = sum((y_predict-bs_ytrue)**2)\n",
    "    #    return bs\n",
    "    def brier_score_singular(y_test,y_predict):\n",
    "        #print(y_predict)\n",
    "        #print(y_test)\n",
    "        obj_num = np.size(y_test)\n",
    "        bs_ytrue = np.zeros((obj_num,nclass))\n",
    "        for i in range(obj_num):\n",
    "            bs_ytrue[i,y_test[i]]=1\n",
    "        bs = sum(sum((y_predict-bs_ytrue)**2))/obj_num\n",
    "        return bs\n",
    "    \n",
    "    return brier_score_singular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TMTB_and_ECE(y_predict, y_test, nclass, nbins=10, ccstrat='uniform'):\n",
    "    y_pred_list = np.reshape(y_predict,nclass*y_test.size)\n",
    "    ninst = y_test.size\n",
    "    onehot = np.zeros((ninst, nclass))\n",
    "    onehot[np.arange(ninst), y_test] = 1\n",
    "    y_test_list = np.reshape(onehot,nclass*y_test.size)\n",
    "    prob_true, prob_pred = calibration_curve(y_test_list, y_pred_list, n_bins=nbins, strategy=ccstrat)\n",
    "    ece = np.sum(np.absolute(prob_true-prob_pred))/prob_true.size\n",
    "    return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Compare all models\n",
    "def CompareModels(data_X, data_y, n_runs=1):\n",
    "    seed = 2000\n",
    "    num_classes = np.unique(data_y).size\n",
    "    # for each run the brierscore for each model [no calibration]\n",
    "    bs_byModel_base_run = []\n",
    "    # for each run the ece (expected calibration error) for each model [no calibration]\n",
    "    ece_byModel_base_run = []\n",
    "    # for each run the brierscore for each model [sigmoid calibration]\n",
    "    bs_byModel_sig_run = []\n",
    "    # for each run the ece (expected calibration error) for each model [sigmoid calibration]\n",
    "    ece_byModel_sig_run = []\n",
    "    # for each run the brierscore for each model [isotonic calibration]\n",
    "    bs_byModel_iso_run = []\n",
    "    # for each run the ece (expected calibration error) for each model [isotonic calibration]\n",
    "    ece_byModel_iso_run = []\n",
    "    # for each run the hyperparameter for each model\n",
    "    hyperparam_byModel_run = []\n",
    "    # for each run the mean fit time for each model\n",
    "    mft_byModel_run = []\n",
    "    # for each run the mean score time for each model\n",
    "    mst_byModel_run = []\n",
    "    # RUNS\n",
    "    for i in range(n_runs):\n",
    "        print('run',i+1)\n",
    "        bs_byModel_base_run.append([])\n",
    "        ece_byModel_base_run.append([])\n",
    "        bs_byModel_sig_run.append([])\n",
    "        ece_byModel_sig_run.append([])\n",
    "        bs_byModel_iso_run.append([])\n",
    "        ece_byModel_iso_run.append([])\n",
    "        hyperparam_byModel_run.append([])\n",
    "        mft_byModel_run.append([])\n",
    "        mst_byModel_run.append([])\n",
    "        # train - test von data (80/20)\n",
    "        X_train, X_test, y_train, y_test = tts(data_X, data_y, test_size=0.2, stratify=data_y, random_state=seed)\n",
    "        # model - calibration split von train (70/30)\n",
    "        X_model, X_calibration, y_model, y_calibration = tts(X_train, y_train, test_size=0.3, stratify=y_train, random_state=seed+1)\n",
    "        \n",
    "        scoring = {'bs': make_scorer(brier_score_singular_factory(num_classes), greater_is_better=False, needs_proba=True)}\n",
    "        \n",
    "        model_dists = generate_ModelHyperparam_pairs(num_classes, seed)\n",
    "        \n",
    "        for k in range(len(model_dists)):\n",
    "            print('model',k+1)\n",
    "            #if (k==1):\n",
    "            #    set_trace()\n",
    "            model_rs = RandomizedSearchCV(model_dists[k][0], param_distributions=model_dists[k][1], scoring=scoring,refit='bs', n_iter = 10, n_jobs = 10, cv=3, random_state=seed)\n",
    "            model_rs.fit(X_model, y_model)\n",
    "            print('mean fit time:',model_rs.cv_results_['mean_fit_time'].mean())\n",
    "            print('mean score time:',model_rs.cv_results_['mean_score_time'].mean())\n",
    "            seed += 1\n",
    "            print(model_rs.best_params_)\n",
    "            # best estimator\n",
    "            hyperparam_byModel_run[i].append(model_rs.best_params_)\n",
    "            model = model_rs.best_estimator_\n",
    "            #calibration sigmoid\n",
    "            c_sig_model = CalibratedClassifierCV(base_estimator=model,method='sigmoid', cv='prefit')\n",
    "            c_sig_model.fit(X_calibration, y_calibration)\n",
    "            #calibration isotonic\n",
    "            c_iso_model = CalibratedClassifierCV(base_estimator=model,method='isotonic', cv='prefit')\n",
    "            c_iso_model.fit(X_calibration, y_calibration)\n",
    "            #prediction base\n",
    "            y_pred_base = model.predict_proba(X_test)\n",
    "            bs_base = brier_score(y_predict=y_pred_base, y_test=y_test, nclass=num_classes)\n",
    "            ece_base = TMTB_and_ECE(y_predict=y_pred_base, y_test=y_test, nclass=num_classes, nbins=10, ccstrat='uniform')\n",
    "            #prediction sigmoid calibrated model\n",
    "            y_pred_sig = c_sig_model.predict_proba(X_test)\n",
    "            bs_sig = brier_score(y_predict=y_pred_sig, y_test=y_test, nclass=num_classes)\n",
    "            ece_sig = TMTB_and_ECE(y_predict=y_pred_sig, y_test=y_test, nclass=num_classes, nbins=10, ccstrat='uniform')\n",
    "            #prediction isotonic calibrated model\n",
    "            y_pred_iso = c_iso_model.predict_proba(X_test)\n",
    "            bs_iso = brier_score(y_predict=y_pred_iso, y_test=y_test, nclass=num_classes)\n",
    "            ece_iso = TMTB_and_ECE(y_predict=y_pred_iso, y_test=y_test, nclass=num_classes, nbins=10, ccstrat='uniform')\n",
    "            #append results\n",
    "            bs_byModel_base_run[i].append(bs_base)\n",
    "            ece_byModel_base_run[i].append(ece_base)\n",
    "            bs_byModel_sig_run[i].append(bs_sig)\n",
    "            ece_byModel_sig_run[i].append(ece_sig)\n",
    "            bs_byModel_iso_run[i].append(bs_iso)\n",
    "            ece_byModel_iso_run[i].append(ece_iso)\n",
    "            mft_byModel_run[i].append(model_rs.cv_results_['mean_fit_time'].mean())\n",
    "            mst_byModel_run[i].append(model_rs.cv_results_['mean_score_time'].mean())\n",
    "            print(bs_base,ece_base,bs_sig, ece_sig, bs_iso, ece_iso)\n",
    "    return bs_byModel_base_run, ece_byModel_base_run, bs_byModel_sig_run, ece_byModel_sig_run, bs_byModel_iso_run, ece_byModel_iso_run, hyperparam_byModel_run, mft_byModel_run, mst_byModel_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelcombos.END_DT import EnsembleND_DT as END_DT\n",
    "from modelcombos.END_LR import EnsembleND_LR as END_LR\n",
    "from modelcombos.END_NB import EnsembleND_NB as END_NB\n",
    "from modelcombos.PC_DT import PairwiseCoupling_DT as PC_DT\n",
    "from modelcombos.PC_LR import PairwiseCoupling_LR as PC_LR\n",
    "from modelcombos.PC_NB import PairwiseCoupling_NB as PC_NB\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "def generate_ModelHyperparam_pairs(nclasses, seed):\n",
    "    models = []\n",
    "    RFC_paramdist =  {\n",
    "        'min_impurity_decrease': uniform(0.00001, 0.1),\n",
    "        'min_samples_leaf': randint(1,51)}\n",
    "    models.append((RandomForestClassifier(n_estimators=45, random_state=seed), RFC_paramdist))\n",
    "    DT_paramdist = {\n",
    "        'max_depth': randint(1,4)\n",
    "    }\n",
    "    LR_paramdist = {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': uniform(0.01,20)\n",
    "    }\n",
    "    NB_paramdist = {\n",
    "        'var_smoothing' : uniform(0.0000000001,1.0)\n",
    "    }\n",
    "    models.append((END_DT(number_of_nds=5, number_of_classes=nclasses, max_depth = 1, generator_String='random_pair', random_state=seed), DT_paramdist))\n",
    "    models.append((END_LR(number_of_nds=5, number_of_classes=nclasses,penalty='l2', C=1.0, generator_String='random_pair', random_state=seed), LR_paramdist))\n",
    "    models.append((END_NB(number_of_nds=5, number_of_classes=nclasses, var_smoothing=0.00001, generator_String='random_pair', random_state=seed),NB_paramdist))\n",
    "    models.append((PC_DT(classes=nclasses, seed=seed, max_depth=1),DT_paramdist))\n",
    "    models.append((PC_LR(classes=nclasses, seed=seed, penalty='l2', C=1.0),LR_paramdist))\n",
    "    models.append((PC_NB(classes=nclasses, seed=seed, var_smoothing=0.0001),NB_paramdist))\n",
    "    MLP_paramdist = {\n",
    "        'alpha': uniform(0.00000001, 10),\n",
    "        'batch_size': [100,200,300,400,500],\n",
    "        'power_t': uniform(0.001,5) \n",
    "    }\n",
    "    models.append((MLPClassifier(hidden_layer_sizes = (35,), activation='logistic', learning_rate='invscaling'), MLP_paramdist))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "    \n",
    "def write_single_value_per_run(file, models, value_name, values):\n",
    "    file.write(value_name+'\\n')\n",
    "    file.write(generateModelName(models[0][0]))\n",
    "    for i in range(len(models)-1):\n",
    "        file.write('\\t'+generateModelName(models[i+1][0]))\n",
    "    file.write('\\n')\n",
    "    for i in range(len(values)):\n",
    "        file.write(str(values[i][0]))\n",
    "        for k in range(len(values[i])-1):\n",
    "            file.write('\\t'+str(values[i][k+1]))\n",
    "        file.write('\\n')\n",
    "    \n",
    "def save_result(dataset_name, nclasses, bs_byModel_base_run, ece_byModel_base_run, bs_byModel_sig_run, ece_byModel_sig_run, bs_byModel_iso_run, ece_byModel_iso_run, hyperparam_byModel_run, mft_byModel_run, mst_byModel_run):\n",
    "    dir_path = os.getcwd()\n",
    "    directory = dir_path+'/experiments/'+dataset_name+'/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    models = generate_ModelHyperparam_pairs(nclasses=nclasses, seed=42)\n",
    "    file = open(directory+'result_'+strftime(\"%Y-%m-%d %H_%M_%S\", gmtime())+'.txt', 'w')\n",
    "    # WRITE MODEL DESCRIPTIONS\n",
    "    try:\n",
    "        # SAVING BRIER SCORE\n",
    "        file.write('Brier-Score\\n')\n",
    "        file.write(generateModelName(models[0][0]))\n",
    "        for i in range(len(models)-1):\n",
    "            file.write('\\t\\t\\t'+generateModelName(models[i+1][0]))\n",
    "        file.write('\\nBase\\tSigmoid\\tIsotonic')\n",
    "        for i in range(len(models)-1):\n",
    "            file.write('\\tBase\\tSigmoid\\tIsotonic')\n",
    "        file.write('\\n')\n",
    "        for i in range(len(bs_byModel_base_run)):\n",
    "            file.write(str(bs_byModel_base_run[i][0])+'\\t'+str(bs_byModel_sig_run[i][0])+'\\t'+str(bs_byModel_iso_run[i][0]))\n",
    "            for k in range(len(bs_byModel_base_run[i])-1):\n",
    "                file.write('\\t'+str(bs_byModel_base_run[i][k+1])+'\\t'+str(bs_byModel_sig_run[i][k+1])+'\\t'+str(bs_byModel_iso_run[i][k+1]))\n",
    "            file.write('\\n')\n",
    "        # SAVING ECE SCORE\n",
    "        file.write('ECE-Score\\n')\n",
    "        file.write(generateModelName(models[0][0]))\n",
    "        for i in range(len(models)-1):\n",
    "            file.write('\\t\\t\\t'+generateModelName(models[i+1][0]))\n",
    "        file.write('\\nBase\\tSigmoid\\tIsotonic')\n",
    "        for i in range(len(models)-1):\n",
    "            file.write('\\tBase\\tSigmoid\\tIsotonic')\n",
    "        file.write('\\n')\n",
    "        for i in range(len(ece_byModel_base_run)):\n",
    "            file.write(str(ece_byModel_base_run[i][0])+'\\t'+str(ece_byModel_sig_run[i][0])+'\\t'+str(ece_byModel_iso_run[i][0]))\n",
    "            for k in range(len(ece_byModel_base_run[i])-1):\n",
    "                file.write('\\t'+str(ece_byModel_base_run[i][k+1])+'\\t'+str(ece_byModel_sig_run[i][k+1])+'\\t'+str(ece_byModel_iso_run[i][k+1]))\n",
    "            file.write('\\n')\n",
    "        # SAVING HYPERPARAMETER\n",
    "        write_single_value_per_run(file, models, 'Hyperparameter', hyperparam_byModel_run)\n",
    "        # SAVING MEAN FIT TIME\n",
    "        write_single_value_per_run(file, models, 'Mean Fit Time', mft_byModel_run)\n",
    "        # SAVING MEAN SCORE TIME\n",
    "        write_single_value_per_run(file, models, 'Mean Score Time', mst_byModel_run)\n",
    "    finally:\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial-characters\n",
      "run 1\n",
      "model 1\n",
      "mean fit time: 0.09446362654368082\n",
      "mean score time: 0.03242847919464111\n",
      "{'min_impurity_decrease': 0.006820628846711264, 'min_samples_leaf': 28}\n",
      "0.7095073617797227 0.25811497790074217 0.6242072667790545 0.10497712751846511 0.5952020868208618 0.0803100773111178\n",
      "model 2\n",
      "mean fit time: 0.35942945480346683\n",
      "mean score time: 0.028005568186442058\n",
      "{'max_depth': 3}\n",
      "0.6827938327880142 0.10365108940146778 0.6688646806144374 0.05800969948473712 0.6458725559069057 0.04825468928843779\n",
      "model 3\n",
      "mean fit time: 2.30243178208669\n",
      "mean score time: 0.016038695971171062\n",
      "{'C': 1.989002315967564, 'penalty': 'l1'}\n",
      "0.7503205215511878 0.02168301256888665 0.7565785313345491 0.054110694531687764 0.7323179960485989 0.13685765558655918\n",
      "model 4\n",
      "mean fit time: 0.3946847677230835\n",
      "mean score time: 0.060223897298177086\n",
      "{'var_smoothing': 0.18836204849738158}\n",
      "0.8044692683539403 0.04989852167859649 0.7916972685537925 0.058089575705687034 0.772993405367868 0.060599620660182124\n",
      "model 5\n",
      "mean fit time: 0.06372719605763752\n",
      "mean score time: 16.649616607030232\n",
      "{'max_depth': 3}\n",
      "0.6214641768339789 0.0365008058950262 0.6125963074753206 0.050364160241444064 0.5863481945940198 0.03205354435504444\n",
      "model 6\n",
      "mean fit time: 1.0602871735890707\n",
      "mean score time: 9.25520281791687\n",
      "{'C': 15.548274001597507, 'penalty': 'l1'}\n",
      "0.6517117275160911 0.036611743766948826 0.6685125740118985 0.07606493981997604 0.6358543658904159 0.03687183715972327\n",
      "model 7\n",
      "mean fit time: 0.057397850354512524\n",
      "mean score time: 20.800758202870686\n",
      "{'var_smoothing': 0.2415552583635281}\n",
      "0.8001535085412183 0.049182385743187326 0.7872465391879299 0.05256333416043377 0.7691838454263452 0.05840094825534216\n",
      "model 8\n",
      "mean fit time: 3.893817019462586\n",
      "mean score time: 0.005689287185668945\n",
      "{'alpha': 0.9869121640622517, 'batch_size': 100, 'power_t': 2.3297820101364564}\n",
      "0.6054930282524714 0.1509352218756404 0.5667811964312277 0.06142230002037168 0.5489972094707037 0.04414681605169223\n"
     ]
    }
   ],
   "source": [
    "#1515 - micro-mass\n",
    "#1459 - artificial-characters\n",
    "#1233 - eating\n",
    "#1569 - poker-hand\n",
    "#1503 - spoken-arabic-digit\n",
    "#4541 - Diabetes130US\n",
    "#4538 - GesturePhaseSegmentationProcessed\n",
    "#41991 - Kuzushiji-49 - error\n",
    "#40670 - dna\n",
    "#1478 - har\n",
    "#40984 - segment\n",
    "#40498 - wine-quality-white\n",
    "#40499 - texture\n",
    "#40686 - solar-flare\n",
    "#41972 - Indian_pines\n",
    "#1475 - first-order-theorem-proving\n",
    "dataset = openml.datasets.get_dataset(1459)\n",
    "dataset_name = dataset.name\n",
    "print(dataset_name)\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(dataset_format='array', target=dataset.default_target_attribute)\n",
    "num_classes = np.unique(y).size\n",
    "#bs & ece for (b)ase, (s)igmoid and (i)sotonic\n",
    "bs_b, ece_b, bs_s, ece_s, bs_i, ece_i, hyppar, mft, mst = CompareModels(X,y, 1)\n",
    "save_result(dataset_name=dataset_name, nclasses=num_classes, bs_byModel_base_run=bs_b, ece_byModel_base_run=ece_b, bs_byModel_sig_run=bs_s, ece_byModel_sig_run=ece_s, bs_byModel_iso_run=bs_i, ece_byModel_iso_run=ece_i, hyperparam_byModel_run=hyppar, mft_byModel_run=mft, mst_byModel_run=mst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
